{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ITNPDB2 Representing and Manipulating Data</h1>\n",
    "<h3>University of Stirling<br>Dr. Saemundur Haraldsson</h3>\n",
    "<h2>Text processing</h2>\n",
    "<h4>        \n",
    "    <ul>\n",
    "        <li>Natural language processing</li>\n",
    "        <ul>\n",
    "            <li>\n",
    "                Lexical analysis (Tokenization)\n",
    "            </li>\n",
    "            <li>Natural Language Toolkit <a href=\"http://www.nltk.org/book/\">(Useful book)</a><br>\n",
    "                Vader -- sentiment analysis\n",
    "            </li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical analysis or Tokenization\n",
    "### is the process of converting a string or text into tokens <br> Paragraphs -> Sentences -> Clauses  -> Words\n",
    "\n",
    "#### Depending on the purpose we include/exclude:   \n",
    "    - punctuations (. , ; : )\n",
    "    - white space\n",
    "    - others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We read in the whole document\n",
    "with open('data/for_lexical.txt','r') as fid:\n",
    "    text = fid.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraphs (in our text) will be delimited by 2 newline characters\n",
    "- This usually has to be checked manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = text.split('\\n\\n')\n",
    "display(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We notice that there are a lot of extra newline characters \\n in each paragraph <br> we don't need them\n",
    "- We'll use __replace__ to exclude them from our tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = [i.replace('\\n',' ') for i in paragraphs]\n",
    "display(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences are usually delimited by full stops\n",
    "- and as sentences are a smaller entity than paragraphs we can work from there\n",
    "- watch out though, splitting each paragrap gives us a separate list. We'll use list comprehension to flatten the resulting list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [i for j in paragraphs for i in j.split('.')]\n",
    "display(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences are made up of clauses\n",
    "- There are a number of grammatical rules to define a clause but we'er going to simplify it a bit for this exercise\n",
    "- Let's define a clause as anything that comes before or after the following conjunctions:\n",
    " - __and__\n",
    " - __or__\n",
    " - __but__\n",
    " - __nor__\n",
    "\n",
    "### Working onwards from sentences we split each into clauses\n",
    "- __Not grammatically correct__ as we're not concerned with that at the moment\n",
    "- __split__ only takes one argument but we want to split on any occurance of our conjunctions which are many\n",
    "- we can iterate through the sentences and make an overly complicated __if then else__ script\n",
    "- or we can use __re__, python's regular expression package\n",
    "- we still use list comprehension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "clauses = [i for j in sentences for i in re.split(r' and | or | but | nor ',j)]\n",
    "# This is very simplified regular expression\n",
    "display(clauses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And lastly we want the words\n",
    "- we could split the entire text on white spaces as in __text.split(' ')__\n",
    " - That would give us everything but the white spaces, i.e. leaves in the punctuation\n",
    "- Simplest way is to use __re.split()__ and split on words \n",
    "    - __\\W__ is a predifined special sequence for any character that is not a word character\n",
    "    - __\\s__ is white space\n",
    "    - __re.split()__ will __not__ treat consecutive separators as a single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = re.split(r'\\W\\s*|\\s',text) # This will also split hyphenated words and words with Apostrophes\n",
    "display(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now what to do with these tokens?\n",
    "### We can count the number of unique tokens in each token class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of paragraphs is {}, thereof unique {}\".format(len(paragraphs),len(set(paragraphs))))\n",
    "print(\"Number of sentences is {}, thereof unique {}\".format(len(sentences),len(set(sentences))))\n",
    "print(\"Number of clauses is {}, thereof unique {}\".format(len(clauses),len(set(clauses))))\n",
    "print(\"Number of words is {}, thereof unique {}\".format(len(words),len(set(words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's count occurrences of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save it in a list of tuples\n",
    "word_count = [(word,words.count(word)) for word in set(words)]\n",
    "display(word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see a few of the most frequent words\n",
    "- couple of things happening in the following line\n",
    "- but only in a single function call\n",
    " - we're using the __key__ and __reverse__ keyword arguments for sorted\n",
    " - and only displaying the first 15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sorted(word_count,key=lambda x:x[1],reverse=True)[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Tokenization is sometimes a preprocess for plagiarism detection.<br>\n",
    "    Words as tokens would not be very useful for that task.<br>\n",
    "    Clauses, sentences, and paragraphs are rather language specific ways.\n",
    "</h2>\n",
    "\n",
    "### A really simple* plagiarism technique would be to split into overlapping set lenght \"word windows\"\n",
    "- Let's try it out with every pairs of 7 words.\n",
    "- We don't need white space characters so we'll use the word list which is in the correct order already\n",
    " - but we need to clean it a bit first\n",
    " - there are some whitespaces.\n",
    " - we also know that any single instance of __s__ should be part of the word before (right?)\n",
    " - can you think of any other obvious things we should look out for as well?\n",
    "- We could use functions and tools from __itertools__ https://docs.python.org/3.7/library/itertools.html\n",
    "- Or we can make our own one-liner\n",
    "\n",
    "*This is far from state-of-the-art, actually would probably not really work in praxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans out whitespaces:\n",
    "clean_words = [i.strip() for i in words if len(i.strip())>0] \n",
    "# find the single s character:\n",
    "ind_of_s = [ind for ind,w in enumerate(clean_words) if w=='s'] \n",
    "# Add the apostrophe and s:\n",
    "clean_words = [word+\"Â´s\" if ind+1 in ind_of_s else word for ind,word in enumerate(clean_words) ] \n",
    "ind_of_s.reverse() # Why do you think we're reversing the index list?\n",
    "# Remove the single s characters:\n",
    "for ind in ind_of_s: \n",
    "    del(clean_words[ind])\n",
    "\n",
    "# Now we'll make our list of 7-grams from the cleaned list of words:\n",
    "seven_grams = [' '.join(clean_words[i:i+6]) for i in range(len(clean_words) - (6))]\n",
    "display(seven_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "## Natural Language Toolkit\n",
    "- There's a package for what we were doing\n",
    "- __and so much more__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First some imports that we'll need\n",
    "import nltk\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(nltk.tokenize.sent_tokenize(text)) # sentence tokens\n",
    "display(nltk.word_tokenize(text)) # word/type tokens, includes punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's load some data that is more interesting than we had before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The previous cell loaded a bunch of books into memory as NLTK text objects\n",
    "- They provide some interesting methods \n",
    "- Let's explore it a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can do a simple wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(text2))\n",
    "display(len(set(text2)))\n",
    "display(len(text2)/len(set(text2))) # Lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(text1) # What is in text1?\n",
    "display(dir(text1)) # What attributes can we see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can search for words and their context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.concordance('honour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for words in similar context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.similar('honour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the similar contexts of two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.common_contexts(['very','monstrous'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise where words appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "text2.dispersion_plot(['honour','decent','lovely'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distribution of the 50 most frequent tokens\n",
    "FreqDist() is part of the nltk.book module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(text2)\n",
    "vocabulary = sorted(list(fdist.items()),key=lambda x:x[1],reverse=True)\n",
    "display(vocabulary[:50])\n",
    "plt.figure(figsize=(12,12))\n",
    "fdist.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That wouldn't tell us much about the text, what about the most infrequent words?\n",
    "- Those that only occur once\n",
    "- __hapaxes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many are they?\n",
    "len(fdist.hapaxes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay, that's a bit too many rare words to tell us anything of importance\n",
    "## Let's try something else\n",
    "- Do you know what we're doing here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_words = sorted([w for w in set(text2) if len(w) > 11 and fdist[w] > 7])\n",
    "display(interesting_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualise where the words found above are in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "text2.dispersion_plot(interesting_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also see commonly co-occurring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.collocations(num=20,window_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis using VADER\n",
    "- Attempts to extract opinions or \"feelings\" from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarity scores estimate the sentiment of text\n",
    "- __neg__: Negative sentiment between 0 and 1\n",
    "- __pos__: Positive sentiment between 0 and 1\n",
    "- __neu__: Neutral sentiment between 0 and 1\n",
    "- __compound__: Normalised sum of the three above between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = analyser.polarity_scores(\"This is an awful sentence that I've written.\")\n",
    "display(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check the sentiment for the first 100 sentences of book 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2_sentences = nltk.sent_tokenize(' '.join(text2))\n",
    "score = analyser.polarity_scores(' '.join(text2_sentences[:100]))\n",
    "display(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text2_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we plot how the sentiment evolves per sentence throughout the book?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [analyser.polarity_scores(sent)['compound'] for sent in text2_sentences]#[0::50]]\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.plot(scores)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
