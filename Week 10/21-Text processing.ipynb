{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ITNPDB2 Representing and Manipulating Data</h1>\n",
    "<h3>University of Stirling<br>Dr. Saemundur Haraldsson</h3>\n",
    "<h2>Text processing</h2>\n",
    "<h4>        \n",
    "    <ul>\n",
    "        <li>Natural language processing</li>\n",
    "        <ul>\n",
    "            <li>\n",
    "                Lexical analysis (Tokenization)\n",
    "            </li>\n",
    "            <li>Natural Language Toolkit <a href=\"http://www.nltk.org/book/\">(Useful book)</a><br>\n",
    "                Vader -- sentiment analysis\n",
    "            </li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical analysis or Tokenization\n",
    "### is the process of converting a string or text into tokens <br> Paragraphs -> Sentences -> Clauses  -> Words\n",
    "\n",
    "#### Depending on the purpose we include/exclude:   \n",
    "    - punctuations (. , ; : )\n",
    "    - white space\n",
    "    - others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# We read in the whole document\n",
    "with open('for_lexical.txt','r') as fid:\n",
    "    text = fid.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraphs (in our text) will be delimited by 2 newline characters\n",
    "- This usually has to be checked manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paragraphs = text.split('\\n\\n')\n",
    "display(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We notice that there are a lot of extra newline characters \\n in each paragraph <br> we don't need them\n",
    "- We'll use __replace__ to exclude them from our tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From's the northern boundary to Concepción, the coast line is generally uniform and indentations are rare. There are only a few bays of any considerable size, and only an occasional cape or promontory. From Chiloé to Tierra del Fuego is a stretch of coast five hundred miles in length, which a glance at the map will show is a perfect network of islands, peninsulas and channels. This is the Chilean Patagonia. It provides scenery as grandly picturesque as the famous fiords along the coast of Norway, and greatly resembles that broken and rugged coast. The bays and gulfs cut into the shores to the foothills of the Andean range. They are of great depth. The Gulf of Las Peñas furnishes an entrance to this labyrinth at the north, and the Straits of Magellan at the south. Some of the passes are so narrow that they seem like gigantic splits in the mountain ranges—grandly gloomy and narrow. Through these openings in the rock the water rushes with terrific force owing to the action of the tides. But, once within, the opening broadens out into little bays, where the waters are as calm and serene as a mountain lake. These channels are a vast Campo Santo, or God’s Acre, of wrecked vessels. Numerous as the disasters have been the sight of a stranded boat is rare, for the grave is usually hundreds of fathoms deep. In every case, however, the wrecked vessel has given her name to the rock that brought disaster, and the official charts are dotted with the names of rocks, which thus form eternal headstones for the unfortunate vessels. One writer has given the following account of these channels:—\",\n",
       " '“If one can imagine the Hudson River bordered continuously by verdure-covered mountains descending precipitously into the water, and jutting out here and there in fantastic buttress-like headlands, one has some idea of Messier Channel. But add to this a network of long, thin cataracts threading their way thousands of feet down through gullies and alleys from mountain crest to water edge. Far up the mountain sides they are so distant as to seem motionless, like threads of silver beaten into the crevices of the rocks; but near the water their motion can be both seen and heard as they fall amid the rocks to reach the sea.”']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paragraphs = [i.replace('\\n',' ') for i in paragraphs]\n",
    "display(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences are usually delimited by full stops\n",
    "- and as sentences are a smaller entity than paragraphs we can work from there\n",
    "- watch out though, splitting each paragrap gives us a separate list. We'll use list comprehension to flatten the resulting list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From's the northern boundary to Concepción, the coast line is generally uniform and indentations are rare\",\n",
       " ' There are only a few bays of any considerable size, and only an occasional cape or promontory',\n",
       " ' From Chiloé to Tierra del Fuego is a stretch of coast five hundred miles in length, which a glance at the map will show is a perfect network of islands, peninsulas and channels',\n",
       " ' This is the Chilean Patagonia',\n",
       " ' It provides scenery as grandly picturesque as the famous fiords along the coast of Norway, and greatly resembles that broken and rugged coast',\n",
       " ' The bays and gulfs cut into the shores to the foothills of the Andean range',\n",
       " ' They are of great depth',\n",
       " ' The Gulf of Las Peñas furnishes an entrance to this labyrinth at the north, and the Straits of Magellan at the south',\n",
       " ' Some of the passes are so narrow that they seem like gigantic splits in the mountain ranges—grandly gloomy and narrow',\n",
       " ' Through these openings in the rock the water rushes with terrific force owing to the action of the tides',\n",
       " ' But, once within, the opening broadens out into little bays, where the waters are as calm and serene as a mountain lake',\n",
       " ' These channels are a vast Campo Santo, or God’s Acre, of wrecked vessels',\n",
       " ' Numerous as the disasters have been the sight of a stranded boat is rare, for the grave is usually hundreds of fathoms deep',\n",
       " ' In every case, however, the wrecked vessel has given her name to the rock that brought disaster, and the official charts are dotted with the names of rocks, which thus form eternal headstones for the unfortunate vessels',\n",
       " ' One writer has given the following account of these channels:—',\n",
       " '“If one can imagine the Hudson River bordered continuously by verdure-covered mountains descending precipitously into the water, and jutting out here and there in fantastic buttress-like headlands, one has some idea of Messier Channel',\n",
       " ' But add to this a network of long, thin cataracts threading their way thousands of feet down through gullies and alleys from mountain crest to water edge',\n",
       " ' Far up the mountain sides they are so distant as to seem motionless, like threads of silver beaten into the crevices of the rocks; but near the water their motion can be both seen and heard as they fall amid the rocks to reach the sea',\n",
       " '”']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [i for j in paragraphs for i in j.split('.')]\n",
    "display(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences are made up of clauses\n",
    "- There are a number of grammatical rules to define a clause but we'er going to simplify it a bit for this exercise\n",
    "- Let's define a clause as anything that comes before or after the following conjunctions:\n",
    " - __and__\n",
    " - __or__\n",
    " - __but__\n",
    " - __nor__\n",
    "\n",
    "### Working onwards from sentences we split each into clauses\n",
    "- __Not grammatically correct__ as we're not concerned with that at the moment\n",
    "- __split__ only takes one argument but we want to split on any occurance of our conjunctions which are many\n",
    "- we can iterate through the sentences and make an overly complicated __if then else__ script\n",
    "- or we can use __re__, python's regular expression package\n",
    "- we still use list comprehension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From's the northern boundary to Concepción, the coast line is generally uniform\",\n",
       " 'indentations are rare',\n",
       " ' There are only a few bays of any considerable size,',\n",
       " 'only an occasional cape',\n",
       " 'promontory',\n",
       " ' From Chiloé to Tierra del Fuego is a stretch of coast five hundred miles in length, which a glance at the map will show is a perfect network of islands, peninsulas',\n",
       " 'channels',\n",
       " ' This is the Chilean Patagonia',\n",
       " ' It provides scenery as grandly picturesque as the famous fiords along the coast of Norway,',\n",
       " 'greatly resembles that broken',\n",
       " 'rugged coast',\n",
       " ' The bays',\n",
       " 'gulfs cut into the shores to the foothills of the Andean range',\n",
       " ' They are of great depth',\n",
       " ' The Gulf of Las Peñas furnishes an entrance to this labyrinth at the north,',\n",
       " 'the Straits of Magellan at the south',\n",
       " ' Some of the passes are so narrow that they seem like gigantic splits in the mountain ranges—grandly gloomy',\n",
       " 'narrow',\n",
       " ' Through these openings in the rock the water rushes with terrific force owing to the action of the tides',\n",
       " ' But, once within, the opening broadens out into little bays, where the waters are as calm',\n",
       " 'serene as a mountain lake',\n",
       " ' These channels are a vast Campo Santo,',\n",
       " 'God’s Acre, of wrecked vessels',\n",
       " ' Numerous as the disasters have been the sight of a stranded boat is rare, for the grave is usually hundreds of fathoms deep',\n",
       " ' In every case, however, the wrecked vessel has given her name to the rock that brought disaster,',\n",
       " 'the official charts are dotted with the names of rocks, which thus form eternal headstones for the unfortunate vessels',\n",
       " ' One writer has given the following account of these channels:—',\n",
       " '“If one can imagine the Hudson River bordered continuously by verdure-covered mountains descending precipitously into the water,',\n",
       " 'jutting out here',\n",
       " 'there in fantastic buttress-like headlands, one has some idea of Messier Channel',\n",
       " ' But add to this a network of long, thin cataracts threading their way thousands of feet down through gullies',\n",
       " 'alleys from mountain crest to water edge',\n",
       " ' Far up the mountain sides they are so distant as to seem motionless, like threads of silver beaten into the crevices of the rocks;',\n",
       " 'near the water their motion can be both seen',\n",
       " 'heard as they fall amid the rocks to reach the sea',\n",
       " '”']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "clauses = [i for j in sentences for i in re.split(r' and | or | but | nor ',j)]\n",
    "# This is very simplified regular expression\n",
    "display(clauses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And lastly we want the words\n",
    "- we could split the entire text on white spaces as in __text.split(' ')__\n",
    " - That would give us everything but the white spaces, i.e. leaves in the punctuation\n",
    "- Simplest way is to use __re.split()__ and split on words \n",
    "    - __\\W__ is a predifined special sequence for any character that is not a word character\n",
    "    - __\\s__ is white space\n",
    "    - __re.split()__ will __not__ treat consecutive separators as a single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From',\n",
       " 's',\n",
       " 'the',\n",
       " 'northern',\n",
       " 'boundary',\n",
       " 'to',\n",
       " 'Concepción',\n",
       " 'the',\n",
       " 'coast',\n",
       " 'line',\n",
       " 'is',\n",
       " 'generally',\n",
       " 'uniform',\n",
       " 'and',\n",
       " 'indentations',\n",
       " 'are',\n",
       " 'rare',\n",
       " 'There',\n",
       " 'are',\n",
       " 'only',\n",
       " 'a',\n",
       " 'few',\n",
       " 'bays',\n",
       " 'of',\n",
       " 'any',\n",
       " 'considerable',\n",
       " 'size',\n",
       " 'and',\n",
       " 'only',\n",
       " 'an',\n",
       " 'occasional',\n",
       " 'cape',\n",
       " 'or',\n",
       " 'promontory',\n",
       " 'From',\n",
       " 'Chiloé',\n",
       " 'to',\n",
       " 'Tierra',\n",
       " 'del',\n",
       " 'Fuego',\n",
       " 'is',\n",
       " 'a',\n",
       " 'stretch',\n",
       " 'of',\n",
       " 'coast',\n",
       " 'five',\n",
       " 'hundred',\n",
       " 'miles',\n",
       " 'in',\n",
       " 'length',\n",
       " 'which',\n",
       " 'a',\n",
       " 'glance',\n",
       " 'at',\n",
       " 'the',\n",
       " 'map',\n",
       " 'will',\n",
       " 'show',\n",
       " 'is',\n",
       " 'a',\n",
       " 'perfect',\n",
       " 'network',\n",
       " 'of',\n",
       " 'islands',\n",
       " 'peninsulas',\n",
       " 'and',\n",
       " 'channels',\n",
       " 'This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Chilean',\n",
       " 'Patagonia',\n",
       " 'It',\n",
       " 'provides',\n",
       " 'scenery',\n",
       " 'as',\n",
       " 'grandly',\n",
       " 'picturesque',\n",
       " 'as',\n",
       " 'the',\n",
       " 'famous',\n",
       " 'fiords',\n",
       " 'along',\n",
       " 'the',\n",
       " 'coast',\n",
       " 'of',\n",
       " 'Norway',\n",
       " 'and',\n",
       " 'greatly',\n",
       " 'resembles',\n",
       " 'that',\n",
       " 'broken',\n",
       " 'and',\n",
       " 'rugged',\n",
       " 'coast',\n",
       " 'The',\n",
       " 'bays',\n",
       " 'and',\n",
       " 'gulfs',\n",
       " 'cut',\n",
       " 'into',\n",
       " 'the',\n",
       " 'shores',\n",
       " 'to',\n",
       " 'the',\n",
       " 'foothills',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Andean',\n",
       " 'range',\n",
       " 'They',\n",
       " 'are',\n",
       " 'of',\n",
       " 'great',\n",
       " 'depth',\n",
       " 'The',\n",
       " 'Gulf',\n",
       " 'of',\n",
       " 'Las',\n",
       " 'Peñas',\n",
       " 'furnishes',\n",
       " 'an',\n",
       " 'entrance',\n",
       " 'to',\n",
       " 'this',\n",
       " 'labyrinth',\n",
       " 'at',\n",
       " 'the',\n",
       " 'north',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Straits',\n",
       " 'of',\n",
       " 'Magellan',\n",
       " 'at',\n",
       " 'the',\n",
       " 'south',\n",
       " 'Some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'passes',\n",
       " 'are',\n",
       " 'so',\n",
       " 'narrow',\n",
       " 'that',\n",
       " 'they',\n",
       " 'seem',\n",
       " 'like',\n",
       " 'gigantic',\n",
       " 'splits',\n",
       " 'in',\n",
       " 'the',\n",
       " 'mountain',\n",
       " 'ranges',\n",
       " 'grandly',\n",
       " 'gloomy',\n",
       " 'and',\n",
       " 'narrow',\n",
       " 'Through',\n",
       " 'these',\n",
       " 'openings',\n",
       " 'in',\n",
       " 'the',\n",
       " 'rock',\n",
       " 'the',\n",
       " 'water',\n",
       " 'rushes',\n",
       " 'with',\n",
       " 'terrific',\n",
       " 'force',\n",
       " 'owing',\n",
       " 'to',\n",
       " 'the',\n",
       " 'action',\n",
       " 'of',\n",
       " 'the',\n",
       " 'tides',\n",
       " 'But',\n",
       " 'once',\n",
       " 'within',\n",
       " 'the',\n",
       " 'opening',\n",
       " 'broadens',\n",
       " 'out',\n",
       " 'into',\n",
       " 'little',\n",
       " 'bays',\n",
       " 'where',\n",
       " 'the',\n",
       " 'waters',\n",
       " 'are',\n",
       " 'as',\n",
       " 'calm',\n",
       " 'and',\n",
       " 'serene',\n",
       " 'as',\n",
       " 'a',\n",
       " 'mountain',\n",
       " 'lake',\n",
       " 'These',\n",
       " 'channels',\n",
       " 'are',\n",
       " 'a',\n",
       " 'vast',\n",
       " 'Campo',\n",
       " 'Santo',\n",
       " 'or',\n",
       " 'God',\n",
       " 's',\n",
       " 'Acre',\n",
       " 'of',\n",
       " 'wrecked',\n",
       " 'vessels',\n",
       " 'Numerous',\n",
       " 'as',\n",
       " 'the',\n",
       " 'disasters',\n",
       " 'have',\n",
       " 'been',\n",
       " 'the',\n",
       " 'sight',\n",
       " 'of',\n",
       " 'a',\n",
       " 'stranded',\n",
       " 'boat',\n",
       " 'is',\n",
       " 'rare',\n",
       " 'for',\n",
       " 'the',\n",
       " 'grave',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'hundreds',\n",
       " 'of',\n",
       " 'fathoms',\n",
       " 'deep',\n",
       " 'In',\n",
       " 'every',\n",
       " 'case',\n",
       " 'however',\n",
       " 'the',\n",
       " 'wrecked',\n",
       " 'vessel',\n",
       " 'has',\n",
       " 'given',\n",
       " 'her',\n",
       " 'name',\n",
       " 'to',\n",
       " 'the',\n",
       " 'rock',\n",
       " 'that',\n",
       " 'brought',\n",
       " 'disaster',\n",
       " 'and',\n",
       " 'the',\n",
       " 'official',\n",
       " 'charts',\n",
       " 'are',\n",
       " 'dotted',\n",
       " 'with',\n",
       " 'the',\n",
       " 'names',\n",
       " 'of',\n",
       " 'rocks',\n",
       " 'which',\n",
       " 'thus',\n",
       " 'form',\n",
       " 'eternal',\n",
       " 'headstones',\n",
       " 'for',\n",
       " 'the',\n",
       " 'unfortunate',\n",
       " 'vessels',\n",
       " 'One',\n",
       " 'writer',\n",
       " 'has',\n",
       " 'given',\n",
       " 'the',\n",
       " 'following',\n",
       " 'account',\n",
       " 'of',\n",
       " 'these',\n",
       " 'channels',\n",
       " '',\n",
       " '',\n",
       " 'If',\n",
       " 'one',\n",
       " 'can',\n",
       " 'imagine',\n",
       " 'the',\n",
       " 'Hudson',\n",
       " 'River',\n",
       " 'bordered',\n",
       " 'continuously',\n",
       " 'by',\n",
       " 'verdure',\n",
       " 'covered',\n",
       " 'mountains',\n",
       " 'descending',\n",
       " 'precipitously',\n",
       " 'into',\n",
       " 'the',\n",
       " 'water',\n",
       " 'and',\n",
       " 'jutting',\n",
       " 'out',\n",
       " 'here',\n",
       " 'and',\n",
       " 'there',\n",
       " 'in',\n",
       " 'fantastic',\n",
       " 'buttress',\n",
       " 'like',\n",
       " 'headlands',\n",
       " 'one',\n",
       " 'has',\n",
       " 'some',\n",
       " 'idea',\n",
       " 'of',\n",
       " 'Messier',\n",
       " 'Channel',\n",
       " 'But',\n",
       " 'add',\n",
       " 'to',\n",
       " 'this',\n",
       " 'a',\n",
       " 'network',\n",
       " 'of',\n",
       " 'long',\n",
       " 'thin',\n",
       " 'cataracts',\n",
       " 'threading',\n",
       " 'their',\n",
       " 'way',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'feet',\n",
       " 'down',\n",
       " 'through',\n",
       " 'gullies',\n",
       " 'and',\n",
       " 'alleys',\n",
       " 'from',\n",
       " 'mountain',\n",
       " 'crest',\n",
       " 'to',\n",
       " 'water',\n",
       " 'edge',\n",
       " 'Far',\n",
       " 'up',\n",
       " 'the',\n",
       " 'mountain',\n",
       " 'sides',\n",
       " 'they',\n",
       " 'are',\n",
       " 'so',\n",
       " 'distant',\n",
       " 'as',\n",
       " 'to',\n",
       " 'seem',\n",
       " 'motionless',\n",
       " 'like',\n",
       " 'threads',\n",
       " 'of',\n",
       " 'silver',\n",
       " 'beaten',\n",
       " 'into',\n",
       " 'the',\n",
       " 'crevices',\n",
       " 'of',\n",
       " 'the',\n",
       " 'rocks',\n",
       " 'but',\n",
       " 'near',\n",
       " 'the',\n",
       " 'water',\n",
       " 'their',\n",
       " 'motion',\n",
       " 'can',\n",
       " 'be',\n",
       " 'both',\n",
       " 'seen',\n",
       " 'and',\n",
       " 'heard',\n",
       " 'as',\n",
       " 'they',\n",
       " 'fall',\n",
       " 'amid',\n",
       " 'the',\n",
       " 'rocks',\n",
       " 'to',\n",
       " 'reach',\n",
       " 'the',\n",
       " 'sea',\n",
       " '',\n",
       " '']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = re.split(r'\\W\\s*|\\s',text) # This will also split hyphenated words and words with Apostrophes\n",
    "display(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now what to do with these tokens?\n",
    "### We can count the number of unique tokens in each token class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of paragraphs is {}, thereof unique {}\".format(len(paragraphs),len(set(paragraphs))))\n",
    "print(\"Number of sentences is {}, thereof unique {}\".format(len(sentences),len(set(sentences))))\n",
    "print(\"Number of clauses is {}, thereof unique {}\".format(len(clauses),len(set(clauses))))\n",
    "print(\"Number of words is {}, thereof unique {}\".format(len(words),len(set(words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's count occurrences of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save it in a list of tuples\n",
    "word_count = [(word,words.count(word)) for word in set(words)]\n",
    "display(word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see a few of the most frequent words\n",
    "- couple of things happening in the following line\n",
    "- but only in a single function call\n",
    " - we're using the __key__ and __reverse__ keyword arguments for sorted\n",
    " - and only displaying the first 15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sorted(word_count,key=lambda x:x[1],reverse=True)[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Tokenization is sometimes a preprocess for plagiarism detection.<br>\n",
    "    Words as tokens would not be very useful for that task.<br>\n",
    "    Clauses, sentences, and paragraphs are rather language specific ways.\n",
    "</h2>\n",
    "\n",
    "### A really simple* plagiarism technique would be to split into overlapping set lenght \"word windows\"\n",
    "- Let's try it out with every pairs of 7 words.\n",
    "- We don't need white space characters so we'll use the word list which is in the correct order already\n",
    " - but we need to clean it a bit first\n",
    " - there are some whitespaces.\n",
    " - we also know that any single instance of __s__ should be part of the word before (right?)\n",
    " - can you think of any other obvious things we should look out for as well?\n",
    "- We could use functions and tools from __itertools__ https://docs.python.org/3.7/library/itertools.html\n",
    "- Or we can make our own one-liner\n",
    "\n",
    "*This is far from state-of-the-art, actually would probably not really work in praxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans out whitespaces:\n",
    "clean_words = [i.strip() for i in words if len(i.strip())>0] \n",
    "# find the single s character:\n",
    "ind_of_s = [ind for ind,w in enumerate(clean_words) if w=='s'] \n",
    "# Add the apostrophe and s:\n",
    "clean_words = [word+\"´s\" if ind+1 in ind_of_s else word for ind,word in enumerate(clean_words) ] \n",
    "ind_of_s.reverse() # Why do you think we're reversing the index list?\n",
    "# Remove the single s characters:\n",
    "for ind in ind_of_s: \n",
    "    del(clean_words[ind])\n",
    "\n",
    "# Now we'll make our list of 7-grams from the cleaned list of words:\n",
    "seven_grams = [' '.join(clean_words[i:i+6]) for i in range(len(clean_words) - (6))]\n",
    "display(seven_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "## Natural Language Toolkit\n",
    "- There's a package for what we were doing\n",
    "- __and so much more__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First some imports that we'll need\n",
    "import nltk\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/mateuszzaremba/nltk_data'\n    - '/Users/mateuszzaremba/anaconda3/nltk_data'\n    - '/Users/mateuszzaremba/anaconda3/share/nltk_data'\n    - '/Users/mateuszzaremba/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9b55af97f219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# sentence tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# word/type tokens, includes punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/mateuszzaremba/nltk_data'\n    - '/Users/mateuszzaremba/anaconda3/nltk_data'\n    - '/Users/mateuszzaremba/anaconda3/share/nltk_data'\n    - '/Users/mateuszzaremba/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "display(nltk.tokenize.sent_tokenize(text)) # sentence tokens\n",
    "display(nltk.word_tokenize(text)) # word/type tokens, includes punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's load some data that is more interesting than we had before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The previous cell loaded a bunch of books into memory as NLTK text objects\n",
    "- They provide some interesting methods \n",
    "- Let's explore it a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can do a simple wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(text2))\n",
    "display(len(set(text2)))\n",
    "display(len(text2)/len(set(text2))) # Lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(text1) # What is in text1?\n",
    "display(dir(text1)) # What attributes can we see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can search for words and their context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.concordance('honour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for words in similar context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.similar('honour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the similar contexts of two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.common_contexts(['very','monstrous'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise where words appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "text2.dispersion_plot(['honour','decent','lovely'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distribution of the 50 most frequent tokens\n",
    "FreqDist() is part of the nltk.book module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(text2)\n",
    "vocabulary = sorted(list(fdist.items()),key=lambda x:x[1],reverse=True)\n",
    "display(vocabulary[:50])\n",
    "plt.figure(figsize=(12,12))\n",
    "fdist.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That wouldn't tell us much about the text, what about the most infrequent words?\n",
    "- Those that only occur once\n",
    "- __hapaxes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3a58d5772e71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# How many are they?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fdist' is not defined"
     ]
    }
   ],
   "source": [
    "# How many are they?\n",
    "len(fdist.hapaxes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay, that's a bit too many rare words to tell us anything of importance\n",
    "## Let's try something else\n",
    "- Do you know what we're doing here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7c209dbda749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minteresting_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m11\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteresting_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text2' is not defined"
     ]
    }
   ],
   "source": [
    "interesting_words = sorted([w for w in set(text2) if len(w) > 11 and fdist[w] > 7])\n",
    "display(interesting_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualise where the words found above are in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "text2.dispersion_plot(interesting_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also see commonly co-occurring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.collocations(num=20,window_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis using VADER\n",
    "- Attempts to extract opinions or \"feelings\" from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarity scores estimate the sentiment of text\n",
    "- __neg__: Negative sentiment between 0 and 1\n",
    "- __pos__: Positive sentiment between 0 and 1\n",
    "- __neu__: Neutral sentiment between 0 and 1\n",
    "- __compound__: Normalised sum of the three above between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = analyser.polarity_scores(\"This is an awful sentence that I've written.\")\n",
    "display(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check the sentiment for the first 100 sentences of book 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2_sentences = nltk.sent_tokenize(' '.join(text2))\n",
    "score = analyser.polarity_scores(' '.join(text2_sentences[:100]))\n",
    "display(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text2_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we plot how the sentiment evolves per sentence throughout the book?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [analyser.polarity_scores(sent)['compound'] for sent in text2_sentences]#[0::50]]\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.plot(scores)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
